---
title: 场景题
date: 2024-08-25 22:39:28
tags:
categories:
password: 19980617pyr
---

# 1. 电商平台中订单未支付过期如何实现自动关单?

业务场景：：外卖订单超 30 分钟未支付，则自动取订单；用户注册成功 15 分钟后，发短信息通知用户等等。

### 方案一：**基于定时任务**

1. **实现步骤**

​		写一个定时任务，定期扫描数据库中的订单，如果时间过期，就将其状态更新为关闭即可。

![image-20240825224252447](https://panyuro.oss-cn-beijing.aliyuncs.com/image-20240825224252447.png)

2. **优点**
   - 实现容易，成本低，基本不依赖其他组件

3. **缺点**
   - **时间可能不够精确**。由于定时任务扫描的间隔是固定的，所以可能造成一些订单已经过期了一段时间才被扫描到，订单关闭的时间比正常时间晚一些。
   - **增加了数据库的压力**。随着订单的数量越来越多，扫描的成本也会越来越大，执行时间也会被拉长，可能导致某些应该被关闭的订单迟迟没有被关闭。

4. **适用场景**

​		对时间要求不是很敏感，并且数据量不太多的业务场景。

### 方案二： **基于消息队列延迟消费**

##### 1. **实现步骤**

##### 	1. 订单创建时发送延迟消息：

​		当订单创建成功后，向消息队列发送一条延迟消息，设置延迟时间为订单超时时间（如 30 分钟）。

##### 	2. **消费者处理消息**：

​		1. 延迟消息到期后，消费者接收到消息并检查订单状态。

​		2. 如果订单仍处于“未支付”状态，则执行关单逻辑。

3. **更新订单状态**：
   1. 将订单状态更新为“已关闭”。
   2. 调用库存服务，释放相关商品库存。

##### 2. **优点**

- 实时性强，能够精确控制关单时间。
- 减少对数据库的轮询压力。

##### 3. **缺点**

- 需要引入消息队列（如 RabbitMQ、Kafka），增加了系统复杂性。
- 需要确保消息队列的可靠性和稳定性。

## 方案三：基于 **redis 过期监听**

1. 实现步骤
   - 使用 Redis 的键值存储功能记录订单的状态。
   - 设置一个合理的“订单未支付超时时间”（如 30 分钟或 24 小时）。
   - 利用 Redis 的键过期机制，在订单创建时为每个未支付订单设置一个带有过期时间的键。
   - 当 Redis 键过期时，触发监听器执行关单逻辑。

#### **优点**

1. 实时性强
   - 基于 Redis 的过期事件监听能够精确控制关单时间，避免延迟。
2. 性能高
   - Redis 是内存数据库，读写速度快，适合高频场景。
3. 解耦设计
   - 关单逻辑与订单创建逻辑分离，便于维护和扩展。

#### **缺点**

1. 依赖 Redis
   - 需要引入 Redis 中间件，增加了系统复杂性。
2. 事件丢失风险
   - 如果 Redis 重启或网络中断，可能会导致部分过期事件丢失。可以通过定期扫描 Redis Key 或数据库作为补充机制来解决。



# 2. 如何存储前端埋点之类的海量数据？

对于大部分互联网公司来说，数据量最大的几类数据是：前端埋点数据、监控数据和日志数据。“前端埋点数据”也称为“点击流”，是指在App、小程序和Wb页面上的埋点数据，这些埋点数据主要用于记录用户的行为，比如打开了哪个页面，点击了哪个按钮，在哪个商品上停留了多久等信息。

**数据特点**

- **高并发写入**：前端埋点数据通常由大量用户同时产生，对系统的写入能力要求较高。
- **半结构化数据**：埋点数据可能包含事件类型、时间戳、用户ID、页面路径等字段，但字段数量和格式可能不固定。
- **数据量大**：随着时间推移，数据量会快速增长，需要支持水平扩展。
- **查询需求多样**：可能需要按时间范围、用户行为、设备类型等维度进行聚合分析。

**完整架构设计**

```
前端埋点 -> API网关 -> Kafka -> 分布式存储系统（ClickHouse/HBase/S3）
                                   ↓
                           大数据分析平台（Spark/MaxCompute）
                                   ↓
                            数据可视化（Grafana/Tableau）
```



#### 1. 数据采集

- 前端通过 JavaScript SDK 或第三方工具（如 Google Analytics、神策数据）收集用户行为数据。
- 数据通过 HTTPS 发送到后端 API 网关。

#### **2. 数据缓冲**

- 使用 Kafka 或类似的分布式消息队列暂存数据，缓解后端存储的压力。

#### **3. 数据存储**

- 根据需求选择合适的存储系统：
  - **实时分析**：ClickHouse、InfluxDB。
  - **长期存储**：HBase、Cassandra、S3。
  - **离线分析**：HDFS + Spark。

#### **4. 数据分析**

- 使用大数据计算框架（如 Spark、Flink）对数据进行清洗、聚合和分析。
- 将分析结果存储到关系型数据库或 OLAP 引擎（如 Druid）中，用于报表展示。

# 3. EXCEl导入如果数据量到达百万级如何去做?

### **一、问题分析**

#### **1. 传统方式的问题**

- **内存占用高**：一次性加载百万级数据到内存中会导致内存溢出（OutOfMemoryError）。
- **处理时间长**：Excel 文件解析和数据写入数据库的过程耗时较长，用户体验差。
- **并发问题**：大量用户同时上传文件可能导致服务器负载过高。

#### **2. 需求特点**

- **高效性**：需要快速解析和导入百万级数据。
- **稳定性**：确保系统在高负载下不会崩溃。
- **可扩展性**：支持更大的数据量或更多用户并发。

### **二、解决方案**

#### **1. 分片处理 + 异步任务**

##### **思路**

将 Excel 文件分片处理，并通过异步任务逐步导入数据，避免一次性加载所有数据到内存中。

- 分片处理：接收文件后，使用 Apache POI 或 EasyExcel 解析 Excel 文件。将数据分批读取并存储到临时表或队列中。
- 异步任务：使用 Spring 的 `@Async` 注解将数据导入任务异步化，避免阻塞主线程。

#### **2. 数据库优化**

##### **批量插入**：使用数据库的批量插入功能（如 JDBC 的 `addBatch` 和 `executeBatch`），减少与数据库的交互次数。

##### **临时表**：创建一个临时表用于存储导入的数据，完成后将数据合并到主表。

#### **3. 并发控制**

- 使用 Spring Cloud 的限流组件（如 Resilience4j 或 Sentinel）限制同时上传文件的请求数。

- ##### 队列： 将文件上传请求放入消息队列（如 RabbitMQ 或 Kafka），由消费者逐步处理。

#### **4. 前端进度反馈**

- ##### **WebSocket 实时反馈**：使用 WebSocket 在前端实时显示导入进度。

- ##### **前端进度条显示**：在 Vue 中订阅 WebSocket 消息并更新进度条。
